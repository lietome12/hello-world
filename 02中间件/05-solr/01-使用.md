### 配置

在solr/home下新建core2

```
cd /data/solr6/home
mkdir -p core2
mkdir -p core2/data

cp -r /data/solr6/home/configsets/basic_configs/conf/ ./
```

在solr页面执行 ![image](http://note.youdao.com/noteshare?id=4084e775f8b69e67f5e9f0f690aa308d&sub=5717DBBADCFA4D3585986AA6A7108049&ynotemdtimestamp=1625107244421)

```
core2(目录结构)
  --- conf
  --- data
  --- core.properties (点击 Add Core后生成)
```

如下内容可不写

------

在core2/conf/中新建dataConfig.xml文件

```
<?xml version="1.0" encoding="UTF-8" ?>
<dataConfig>
    <dataSource type="JdbcDataSource" dirver="com.mysql.jdbc.Driver" url="jdbc:mysql://127.0.0.1:3306/test" user="root" password="123456" />
    <document>
        <entity name="user" pk="user" query="SELECT id,name,pass,address FROM user" deltaImportQuery="SELECT id,name,pass,address,updateTime FROM user where id='${dih.delta.id}'" deltaQuery="SELECT id FROM user where updateTime">
            <field column="id" name="id"/>
            <field column="name" name="name"/>
            <field column="address" name="address"/>
            <field column="pass" name="pass"/>
            <field column="updateTime" name="updateTime"/>
        </entity>
    </document>
</dataConfig>
```

在dataConfig.xml同级目录solronfig.xml中加入(在solr中进行导入操作)

```
<requestHandler name="/dataimport" class="org.apache.solr.handler.dataimport.DataImportHandler">
    <lst name="defaults">
        <str name="config">dataConfig.xml</str>
    </lst>
</requestHandler>
```

修改managed-schema(主)

```
<field name="id" type="string" indexed="true" stored="true" required="true" multiValued="false" />
<field name="name" type="string" indexed="true" stored="true"/>
<field name="pass" type="string" indexed="true" stored="true"/>
<field name="address" type="string" indexed="true" stored="true"/>
<field name="updateTime" type="date" indexed="false" stored="true"/>
<uniqueKey>id</uniqueKey>
```

浏览器输入：http://127.0.0.1:8080/solr/core2/dataimport?command=full-import&clean=false&commit=true 执行页面返回无错,表示成功 ![image](http://note.youdao.com/noteshare?id=37c319cc852826b91a2135256bbcb202&sub=1EB9EDD21A074706BF0D3903F071CF49&ynotemdtimestamp=1625107244421)

------

### Ik分词器

IkAnalyzer(下载:ik-analyzer-solr5-5.x.jar)

```
# 将ik相关文件拷贝到WEB-INF/lib目录下
cp ik-analyzer-solr5-5.x.jar /usr/local/tomcat/webapps/solr/WEB-INF/lib/

#修改solrConfig.xml(很多<lib>后边加)
<lib dir="./lib" regex=".*\.jar" />


#修改managed-schema
<fieldType name="text_ik" class="solr.TextField">
    <analyzer type="index" >
      <tokenizer class="org.wltea.analyzer.lucene.IKTokenizerFactory" useSmart="false" conf="ik.conf"/>
      <filter class="solr.StopFilterFactory" ignoreCase="true" words="lang/CNstopwords.txt" />
    </analyzer>
    <analyzer type="query">
      <tokenizer class="org.wltea.analyzer.lucene.IKTokenizerFactory" useSmart="false" conf="ik.conf"/>
      <filter class="solr.StopFilterFactory" ignoreCase="true" words="lang/CNstopwords.txt" />
    </analyzer>
</fieldType>

或



<!-- 我添加的IK分词 -->
<fieldType name="text_ik" class="solr.TextField">   
    <analyzer type="index" isMaxWordLength="false" class="org.wltea.analyzer.lucene.IKAnalyzer"/>   
    <analyzer type="query" isMaxWordLength="true" class="org.wltea.analyzer.lucene.IKAnalyzer"/>   
</fieldType>
然后将要分词的字段,设为text_ik
<field name="pathsummary"      type="text_ik"   indexed="true"  stored="true"  multiValued="false" />
<field name="attr_content"      type="text_ik"   indexed="true"  stored="true"  multiValued="false" />
你可能想让document的一些字段可以多次使用。solr 有一个字段复制机制，可以提交多个不同类型字段集中到一个字段。字段复制主要涉及两个概念，source和destination，一个是要复制的字段，另一个是要复制到哪个字段，以下是个例子：
	
<copyField source="cat" dest="text" maxChars="30000" />

上例中，如果text字段有数据的话，cat字段的内容将被添加到text字段中。maxChars 参数，一个int类型参数，用于限制复制的字符数。
source和destination都支持通配符。以下是一个将所有以 _t 结尾的字段全部复制到text字段中。
	
<copyField source="*_t" dest="text" maxChars="25000" />

其实说的简单一点，比如现在你要查询包涵"Java"的博客， 那么你肯定要查内容，标题是否包含Java，但是solr不能像SQL那样，where tittle like '%Java%'  or  content like '%Java%'.   这个时候copyField就派上用场了，
定义一个新字段，将title和content 复制到这个新字段，索引的时候，直接从这个新字段查询，这样就达到目地了。  这便是copyField的典型应用场景 。
注意：如果dest由多个source构成，就需要将其指定为multiValued。
```

### java

将jar包(`/data/solr6/home/dist`)和mysql驱动加入工程中 solr-solrj-6.5.0.jar以及solrj-lib的所有包

(Add Core: user) 修改conf/managed-schema

```
    <!-- 自定义基本信息字段-->
    <field name="id" type="string" indexed="true" stored="true" required="true" multiValued="false" />
	<field name="name" type="text_ik" indexed="true" stored="true"/>
	<field name="pass" type="string" indexed="true" stored="true"/>
	<field name="address" type="string" indexed="true" stored="true"/>
	<field name="createTime" type="plong" indexed="true" stored="true"/>
	<field name="updateTime" type="plong" indexed="true" stored="true"/>
	
	<!--全文匹配-->
	<field name="full_matching" type="text_ik" indexed="true" stored="false" multiValued="true"/>
	<copyField source="id" dest="full_matching"/>
	<copyField source="name" dest="full_matching"/>
	<copyField source="pass" dest="full_matching"/>
	<copyField source="address" dest="full_matching"/>
	<copyField source="createTime" dest="full_matching"/>
	<copyField source="updateTime" dest="full_matching"/>
	
	
    <!-- docValues are enabled by default for long type so we don't need to index the version field  -->
    <field name="_version_" type="plong" indexed="false" stored="false"/>
    <field name="_root_" type="string" indexed="true" stored="false" docValues="false" />
    <field name="_text_" type="text_general" indexed="true" stored="false" multiValued="true"/>
```

重启tomcat即可

SolrServer.java

```
package com.test;

import org.apache.solr.client.solrj.impl.HttpSolrClient;

public class SolrServer {

	private static HttpSolrClient server = null;
	private final static String SOLR_URL = "http://192.168.254.132:8080/solr/";
	
	public static HttpSolrClient getServer(){
		if(server == null){
			server = new HttpSolrClient(SOLR_URL);
			server.setDefaultMaxConnectionsPerHost(1000);
			server.setMaxTotalConnections(10000);
			server.setConnectionTimeout(60000);
			server.setSoTimeout(60000);
			server.setFollowRedirects(false);
			server.setAllowCompression(true);
		}
		return server;
	}
	
}
```

SolrEntity.java

```
package com.test;

import java.io.Serializable;
import java.util.Date;

import org.apache.solr.client.solrj.beans.Field;

public class SolrEntity implements Serializable {

	/**
	 * 
	 */
	private static final long serialVersionUID = 1L;
	
	@Field
	private String id;
	
	@Field
	private String name;
	
	@Field
	private String pass;
	
	@Field
	private String address;
	
	@Field
	private Long createTime;
	
	@Field
	private Long updateTime;

	@Override
	public String toString() {
		return "SolrEntity [id=" + id + ", name=" + name + ", pass=" + pass
				+ ", address=" + address + ", createTime=" + createTime
				+ ", updateTime=" + updateTime + "]";
	}

	public String getId() {
		return id;
	}

	public void setId(String id) {
		this.id = id;
	}

	public String getName() {
		return name;
	}

	public void setName(String name) {
		this.name = name;
	}

	public String getPass() {
		return pass;
	}

	public void setPass(String pass) {
		this.pass = pass;
	}

	public String getAddress() {
		return address;
	}

	public void setAddress(String address) {
		this.address = address;
	}

	public Long getCreateTime() {
		return createTime;
	}

	public void setCreateTime(Long createTime) {
		this.createTime = createTime;
	}

	public Long getUpdateTime() {
		return updateTime;
	}

	public void setUpdateTime(Long updateTime) {
		this.updateTime = updateTime;
	}
}
```

SolrBean.java

```
package com.test;

import java.util.HashMap;
import java.util.List;
import java.util.Map;

import org.apache.solr.client.solrj.SolrQuery;
import org.apache.solr.client.solrj.impl.HttpSolrClient;
import org.apache.solr.client.solrj.response.QueryResponse;
import org.apache.solr.common.SolrDocumentList;

public class SolrBean {
	
	private static final HttpSolrClient server = SolrServer.getServer();
	
	/**
	 * bean add index
	 * 
	 */
	public static void addIndex(SolrEntity entity){
		
		try{
			server.addBean("user",entity);
			server.commit("user");
		}catch(Exception e){
			e.printStackTrace();
			System.out.println("===============创建索引失败====================");
		}
	}
	
	/**
	 * del add index by query
	 */
	public static void delByQuery(String query){
		
		query = "*.*";
		try{
			server.deleteByQuery("user",query);
			server.commit("user");
		}catch(Exception e){
			System.out.println("=================查询所有异常=============================");
		}
		
	}
	
	/**
	 * del by id
	 */
	public static void delById(String id){
		
		try{
			server.deleteById("user",id);
			server.commit("user");
		}catch(Exception e){
			e.printStackTrace();
			System.out.println("===================根据id删除索引失败========================================");
		}
	}
	
	/**
	 * del all
	 */
	public static void delAll(){
		try{
			server.deleteByQuery("user","*:*");
			server.commit("user");
		}catch(Exception e){
			System.out.println("================删除所有索引失败==================");
		}
		
	}
	
	
	/**
	 * query index
	 */
	public static void queryAll() {
		
		SolrQuery query = new SolrQuery(); 
		query.setQuery("*:*");
		try{
			query.setStart(0);// 开始记录数
			query.setRows(10000);// 总条数 
			QueryResponse queryResponse = server.query("user",query);
			
			List<SolrEntity> results = queryResponse.getBeans(SolrEntity.class);
			for(SolrEntity entity:results){
				System.out.println(entity.toString());
			}
		}catch(Exception e){
			e.printStackTrace();
		}
		
	}
	
	public static void queryByName(String name){
		SolrQuery query = new SolrQuery();
		
		try{
			query.setQuery("name:"+name);
			query.setStart(0);// 开始记录数
			query.setRows(10000);// 总条数 
			QueryResponse queryResponse = server.query("user",query);
			
			List<SolrEntity> results = queryResponse.getBeans(SolrEntity.class);
			for(SolrEntity entity:results){
				System.out.println(entity.toString());
			}
		}catch(Exception e){
			e.printStackTrace();
		}
		
	}
	
	/**
	 * 分词 全文检索
	 */
	public static void query(Map searchMap){
		
		SolrQuery query = new SolrQuery();
		
		try{
			//查询字符串;查所有*.*;根据指定字段查询(如:Name:张三 AND Adress:北京),AND必须大写
//			query.setQuery("*:*");
			//过滤查询;将query查询的结果过滤,如:query=*.*&filterQuery=name:张三
//			query.setFilterQueries("name:张三");
			//指定返回哪些字段内容,用逗号或空格分隔
//			query.setFields("id,name,pass,address");
			
			query.set("df", "full_matching");//df默认查询字段
			String search = (String)searchMap.get("full_matching");//查询字段
			if(isNotNull(search))
				query.setQuery(search);
			else
				query.setQuery("*:*");
			
			/**
			 * sort:排序;格式: sort=<field name> + <desc | asc> [,<field name> + <desc | asc>]...
			 * 例:(score desc,price asc):先 score降序,再price降序
			 */
			/*SortClause sort1 = new SortClause("createTime", ORDER.asc);
			SortClause sort2 = new SortClause("updateTime", ORDER.desc);
			
			List<SortClause> sortList = new ArrayList<SortClause>();
			sortList.add(sort1);sortList.add(sort2);
			query.setSorts(sortList);*/
			
//			query.setQuery("name:张三");//
			
			/*query.setHighlight(true);//高亮
			query.addHighlightField("name");//高亮字段
			query.setHighlightSimplePre("<font color='red'>");//高亮单词前缀
			query.setHighlightSimplePost("</font>");//高亮单词后缀
*/			
			/**
			 * 分页
			 */
			/*Integer pageNo = 1;Integer pageSize = 10;
			if(isNotNull(searchMap.get("pageNo"))){
				pageNo = (Integer)searchMap.get("pageNo");
			}
			if(isNotNull(searchMap.get("pageSize")))
				pageNo = (Integer)searchMap.get("pageSize");
			
			query.setStart((pageNo-1)*pageSize);
			query.setRows(pageSize);
			
			*/
			QueryResponse response = server.query("user",query);
			List<SolrEntity> results = response.getBeans(SolrEntity.class);
			
			SolrDocumentList documentList = response.getResults();
			
			System.out.println(documentList);
			
		}catch(Exception e){
			e.printStackTrace();
		}
		
	}
	
	public static boolean isNotNull(Object field){
		if(field != null || !("").equals(field))
			return true;
		return false;
	}
	
	
	public static void main(String[] args) {
		
		/*SolrEntity entity = new SolrEntity();
		entity.setId(UUID.randomUUID().toString().replace("-", ""));
		entity.setName("白开水");
		entity.setPass("baiiohfjebfmsf");
		entity.setAddress("武汉 东湖");
		entity.setCreateTime(new Date().getTime());
		
		SolrBean.addIndex(entity);*/
		
//		SolrBean.queryAll();
		
//		SolrBean.delById("9c54ec1403bf4426b7b296e406a08e4e");
		
//		SolrBean.delAll();
		
		Map<String,Object> map = new HashMap<String, Object>();
		map.put("full_matching", "开水");
		map.put("pageNo", 1);
		map.put("pageSize", 3);
		SolrBean.query(map);
		
//		SolrBean.queryByName("张三");
		
	}
	
}
```

### 查询参数

##### 1.基本查询

```
q  查询的关键字，此参数最为重要，例如，q=id:1，默认为q=*:*,

fl  指定返回哪些字段，用逗号或空格分隔，注意：字段区分大小写，例如，fl= id,title,sort

start  返回结果的第几条记录开始，一般分页用，默认0开始

rows  指定返回结果最多有多少条记录，默认值为 10，配合start实现分页

sort  排序方式，例如id  desc 表示按照 “id” 降序

wt  (writer type)指定输出格式，有 xml, json, php等

fq  （filter query）过滤查询，提供一个可选的筛选器查询。返回在q查询符合结果中同时符合的fq条件的查询结果，例如：q=id:1&fq=sort:[1 TO 5]，找关键字id为1 的，并且sort是1到5之间的。

df   默认的查询字段，一般默认指定。

qt  （query type）指定那个类型来处理查询请求，一般不用指定，默认是standard。

indent   返回的结果是否缩进，默认关闭，用 indent=true|on 开启，一般调试json,php,phps,ruby输出才有必要用这个参数。

version   查询语法的版本，建议不使用它，由服务器指定默认值。
　　
```

##### 2.检索运算符

```
:  指定字段查指定值，如返回所有值*:*,或 name:"李四"

?  表示单个任意字符的通配

*  表示多个任意字符的通配（不能在检索项的开始使用*或者?符号）

~  表示模糊检索，如检索拼写类似于”roam”的项这样写：roam~将找到形如foam和roams的单词；roam~0.8，检索返回相似度在0.8以上的记录。

AND、|| 、OR、&&  布尔操作符

NOT、!、-（排除操作符,不能单独与检索项使用构成查询）

+  存在操作符，要求符号”+”后的项必须在文档相应的域中存在

( )  用于构成子查询

[]  包含范围检索，如检索某时间段记录，包含头尾，date:[201507 TO 201510]

{}  不包含范围检索，如检索某时间段记录，不包含头尾date:{201507 TO 201510}
```

##### 3.高亮

```
h1  是否高亮，hl=true，表示采用高亮

hl.fl  设定高亮显示的字段，用空格或逗号隔开的字段列表。要启用某个字段的highlight功能，就得保证该字段在schema中是stored。如果该参数未被给出，那么就会高亮默认字段 standard handler会用df参数，dismax字段用qf参数。你可以使用星号去方便的高亮所有字段。如果你使用了通配符，那么要考虑启用hl.requiredFieldMatch选项。

hl.requireFieldMatch   如果置为true，除非用hl.fl指定了该字段，查询结果才会被高亮。它的默认值是false。

hl.usePhraseHighlighter   如果一个查询中含有短语（引号框起来的）那么会保证一定要完全匹配短语的才会被高亮。

hl.highlightMultiTerm   如果使用通配符和模糊搜索，那么会确保与通配符匹配的term会高亮。默认为false，同时hl.usePhraseHighlighter要为true。

hl.fragsize   返回的最大字符数。默认是100.如果为0，那么该字段不会被fragmented且整个字段的值会被返回。
```

###### 4.分组

Facet是Solr的核心搜索功能，主要是导航(Guided Navigation)、参数化查询(Paramatic Search)。Facet的主要好处是在搜索的同时，可以按照Facet条件进行分组统计，给出导航信息，改善搜索体验。

Facet主要分为：Field Facet 和 Date Facet 两大类

**4.1Field Facet**

```
facet 参数字段必须被索引

facet=on 或 facet=true

facet.field  分组的字段

facet.prefix  表示Facet字段前缀

facet.limit   Facet字段返回条数

facet.offict  开始条数,偏移量,它与facet.limit配合使用可以达到分页的效果

facet.mincount  Facet字段最小count,默认为0

facet.missing  如果为on或true,那么将统计那些Facet字段值为null的记录

facet.sort  表示 Facet 字段值以哪种顺序返回 .格式为 true(count)|false(index,lex)，true(count) 表示按照 count 值从大到小排列，false(index,lex) 表示按照字段值的自然顺序 (字母 , 数字的顺序 ) 排列 . 默认情况下为 true(count)
```

**4.2 Date Facet**

对日期类型的字段进行 Facet. Solr 为日期字段提供了更为方便的查询统计方式 .注意 , Date Facet的字段类型必须是 DateField( 或其子类型 ). 需要注意的是 , 使用 Date Facet 时 , 字段名 , 起始时间 , 结束时间 , 时间间隔这 4 个参数都必须提供 .

```
facet.date  该参数表示需要进行 Date Facet 的字段名 , 与 facet.field 一样 , 该参数可以被设置多次 , 表示对多个字段进行 Date Facet.

facet.date.start 起始时间 , 时间的一般格式为 ” 2015-12-31T23:59:59Z”, 另外可以使用 ”NOW”,”YEAR”,”MONTH” 等等 ,

facet.date.end  结束时间

facet.date.gap 时间间隔,如果 start 为 2015-1-1,end 为 2016-1-1，gap 设置为 ”+1MONTH” 表示间隔1 个月 , 那么将会把这段时间划分为 12 个间隔段 .

facet.date.hardend  表示 gap 迭代到 end 时，还剩余的一部分时间段，是否继续去下一个间隔. 取值可以为 true|false, 默认为 false.

例 start 为 2015-1-1,end 为 2015-12-21,gap 为 ”+1MONTH”, 如果hardend 为 false，则，最后一个时间段为 2015-12-1 至 2016-1-1; 反之，如果 hardend 为 true，则，最后一个时间段为 2015-12-1 至 2015-12-21.
```

注意：Facet的字段必须被索引，无需分词，无需存储。无需分词是因为该字段的值代表了一个整体概念，无需存储是因为一般而言用户所关心的并不是该字段的具体值，而是作为对查询结果进行分组的一种手段，给出相关的分组信息，从而改善搜索体验。